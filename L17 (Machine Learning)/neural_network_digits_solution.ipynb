{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Data\n",
    "class MnistData(Dataset):\n",
    "    def __init__(self, digits_data):\n",
    "        self.labels = np.array(OneHotEncoder().fit_transform(digits_data.values[:,0][:,np.newaxis]).todense()).astype(np.single)\n",
    "        \n",
    "        self.imdata = (digits_data.values[:,1:]/255.0).astype(np.single)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.labels.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.labels[index], self.imdata[index]\n",
    "\n",
    "    def visualize(self, index):\n",
    "        plt.imshow(self.imdata[index].reshape(28,28), cmap = plt.cm.bone)\n",
    "\n",
    "        \n",
    "digits_data = pd.read_csv(\"data/mnist_digits/train.csv\")\n",
    "train_df, val_df = train_test_split(digits_data, test_size = 0.3)\n",
    "\n",
    "train_digits = MnistData(train_df)\n",
    "val_digits = MnistData(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the neural network\n",
    "class SingleLayerDigitNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SingleLayerDigitNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'digitnet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2e865fff1604>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_digits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimnum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpred_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdigitnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"True label is {}, nn prediction is {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtrain_digits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'digitnet' is not defined"
     ]
    }
   ],
   "source": [
    "#Try making a test prediction with the untrained network\n",
    "imnum = 11\n",
    "singlelayer_digitnet = SingleLayerDigitNet()\n",
    "\n",
    "label, imdata = train_digits[imnum]\n",
    "\n",
    "pred_label = np.argmax(digitnet(torch.Tensor(imdata)).detach().numpy())\n",
    "print(\"True label is {}, nn prediction is {}\".format(np.argmax(label), pred_label))\n",
    "train_digits.visualize(imnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper class for Neural Network training\n",
    "class TrainingStats(object):\n",
    "    def __init__(self):\n",
    "        self.losses = []\n",
    "        self.accs = []\n",
    "        self.reset_runningdata()\n",
    "        \n",
    "    def reset_runningdata(self):\n",
    "        self.loss_running = 0\n",
    "        self.predictions_running = []\n",
    "        self.labels_running = []\n",
    "    \n",
    "    def add(self, predictions, labels, loss):\n",
    "        self.loss_running += loss\n",
    "        self.predictions_running.append(predictions)\n",
    "        self.labels_running.append(labels)\n",
    "        \n",
    "    def new_epoch(self):\n",
    "        self.losses.append(self.loss_running)\n",
    "        preds = np.hstack(self.predictions_running)\n",
    "        labels = np.hstack(self.labels_running)\n",
    "        \n",
    "        self.accs.append(100*(preds == labels).sum()/len(labels))\n",
    "        self.reset_runningdata()\n",
    "\n",
    "#Network training function\n",
    "def train_network(train_dataloader, val_dataloader, model, loss_func, optimizer, train_params):\n",
    "    trainstats = TrainingStats()\n",
    "    valstats = TrainingStats()\n",
    "    \n",
    "    print(\"Epoch, train loss, train acc, val loss, val acc\")\n",
    "    for epochnum in range(train_params[\"epochs\"]):\n",
    "        #Get validation stats\n",
    "        for val_data in val_dataloader:\n",
    "            val_labels = val_data[0]\n",
    "            val_images = val_data[1]\n",
    "            \n",
    "            val_prediction = model(val_images)\n",
    "            val_loss     = loss_func(val_prediction, val_labels).item()\n",
    "        \n",
    "        valstats.add(np.argmax(val_prediction.detach().numpy(), axis = 1), \n",
    "                     np.argmax(val_labels.detach().numpy(), axis = 1),\n",
    "                     val_loss)\n",
    "        valstats.new_epoch()\n",
    "            \n",
    "        #Train the model\n",
    "        for batch_idx, data_batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            label_batch = data_batch[0]\n",
    "            image_batch = data_batch[1]\n",
    "    \n",
    "            train_prediction = model(image_batch)\n",
    "            train_loss       = loss_func(train_prediction, label_batch)\n",
    "\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            trainstats.add(np.argmax(train_prediction.detach().numpy(), axis = 1),\n",
    "                           np.argmax(label_batch.detach().numpy(), axis = 1),\n",
    "                           train_loss.item())\n",
    "    \n",
    "        trainstats.new_epoch()\n",
    "            \n",
    "        print(\"{} {:.2f} {:.2f}% {:.2f} {:.2f}%\".format(epochnum, \n",
    "                                                        trainstats.losses[-1]/len(training_dataloader.dataset),\n",
    "                                                        trainstats.accs[-1],\n",
    "                                                        valstats.losses[-1]/len(val_dataloader.dataset),\n",
    "                                                        valstats.accs[-1]))\n",
    "    return trainstats, valstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\"epochs\": 25,\n",
    "                \"batch_size\": 5,\n",
    "                \"learningRate\": 0.01}\n",
    "\n",
    "singlelayer_digitnet = SingleLayerDigitNet() #retrain from start\n",
    "\n",
    "training_dataloader = torch.utils.data.DataLoader(train_digits,\n",
    "                                                  batch_size = train_params[\"batch_size\"])\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(val_digits,\n",
    "                                             batch_size = len(val_digits))\n",
    "\n",
    "loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(singlelayer_digitnet.parameters(),\n",
    "                            lr = train_params['learningRate'])\n",
    "\n",
    "trainstats, valstats = train_network(training_dataloader,\n",
    "                                     val_dataloader,\n",
    "                                     singlelayer_digitnet,\n",
    "                                     loss_func,\n",
    "                                     optimizer,\n",
    "                                     train_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_firstlayer_weights(network):    \n",
    "    for m in network.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            weights = m.weight        \n",
    "            break\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=2, ncols=5, figsize=(20, 8),\n",
    "                           subplot_kw={'xticks': [], 'yticks': []})\n",
    "    for i, ax in enumerate(axs.flat):    \n",
    "        ax.imshow(weights[i].detach().numpy().reshape(28, 28), \n",
    "                  plt.cm.bone)\n",
    "\n",
    "#The weights look like the numbers, the model has learned useful and generalizable features!\n",
    "visualize_firstlayer_weights(singlelayer_digitnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a more complicated network with less data\n",
    "\n",
    "train_params = {\"epochs\": 100,\n",
    "                \"batch_size\": 1,\n",
    "                \"learningRate\": 0.01}\n",
    "\n",
    "#Define the neural network\n",
    "class MultiLayerDigitNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiLayerDigitNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 14*14)\n",
    "        self.fc2 = nn.Linear(14*14, 100)\n",
    "        self.fc3 = nn.Linear(100, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "multilayer_digitnet = MultiLayerDigitNet() #retrain from start\n",
    "\n",
    "training_dataloader = torch.utils.data.DataLoader(MnistData(train_df.groupby(\"label\").first().reset_index()),\n",
    "                                                  batch_size = train_params[\"batch_size\"])\n",
    "\n",
    "val_dataloader = torch.utils.data.DataLoader(val_digits,\n",
    "                                             batch_size = len(val_digits))\n",
    "\n",
    "loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.SGD(multilayer_digitnet.parameters(),\n",
    "                            lr = train_params['learningRate'])\n",
    "\n",
    "trainstats, valstats = train_network(training_dataloader,\n",
    "                                     val_dataloader,\n",
    "                                     multilayer_digitnet,\n",
    "                                     loss_func,\n",
    "                                     optimizer,\n",
    "                                     train_params)\n",
    "\n",
    "#The accuracy is very high for the training data but poor for the validation data.\n",
    "#The model is overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The weights are no longer recognizable\n",
    "visualize_firstlayer_weights(multilayer_digitnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
