{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the Titanic machine learning practice exercise! Your job is to build a model to predict whether or not a particular passanger survived the disaster. This is a binary classification problem where the outcome $Y = 1$ if the passanger survived, and $Y = 0$ if not. The goal of this exercise is to get familiar with a typical machine learning model building work-flow, and practice working with data and models.\n",
    "\n",
    "![\"Titanic\"](images/titanic.jpeg)\n",
    "\n",
    "You have the following data about the passengers (some may be missing and you might need to figure out how to guess the missing values)\n",
    "\n",
    "\n",
    "|Variable|\tDefinition\t|Key|\n",
    "| :- |-: | :-: |\n",
    "|survival \t|Survival |\t0 = No, 1 = Yes|\n",
    "|pclass \t|Ticket class| \t1 = 1st, 2 = 2nd, 3 = 3rd|\n",
    "|sex| \tSex \t| |\n",
    "|Age \t|Age in years \t| |\n",
    "|sibsp \t|# of siblings / spouses aboard the Titanic| \t|\n",
    "|parch \t|# of parents / children aboard the Titanic| \t\n",
    "|ticket \t|Ticket number \t| |\n",
    "|fare \t|Passenger fare \t| |\n",
    "|cabin \t|Cabin number| \t    |  \n",
    "|embarked| \tPort of Embarkation| \tC = Cherbourg, Q = Queenstown, S = Southampton|\n",
    "\n",
    "\n",
    "You can build any sort of model you want, but if you are a beginner than you should start with logistic regression, which is a simple yet surprisingly powerful classification model that is important for understanding modern neural network technologies.\n",
    "\n",
    "Logistic regression takes in a set of input data $X \\in \\mathbb{R}^{N_{data} \\times N_{feat}}$ and learns a set of data weights $\\beta \\in \\mathbb{R}^{N_{feat}}$, where \n",
    "$N_{data}, N_{feat}$ are the number of data points and number of predictive features, respectively. You do not have to worry about how the logistic model is trained (at first) for this exercise, because you can use model code from Scikit-learn and simply call the .fit() method. Internally, the model will solve a convex optimization problem that determines $\\beta$ using your data $X$ and your set of outcome lables $Y$.\n",
    "\n",
    "During prediction time, the logistic regression model makes a prediction $\\hat{y_i}$ for a new datapoint (passenger) $x_i$ by the following formula\n",
    "\n",
    "$P(y_i = 1) = \\sigma(x_i \\cdotp \\beta)$\n",
    "\n",
    "where \n",
    " \n",
    "$\\sigma(t) = \\frac{e^t}{1 + e^{-t}}$.\n",
    " \n",
    "Since these predictions are probabilities, you can turn them into hard predictions by using a threshold of 0.5, that is \n",
    "\n",
    "$\\hat{y_i} = 1 , \\quad \\text{where } \\sigma(x_i \\cdotp \\beta) > 0.5$.\n",
    "\n",
    "To complete this task you will need to\n",
    "- Manipulate the data so that it can inputed into the scikit-learn LogisticRegression class. You will need to recode any categorical variables that you want to use (Why is that?). To keep things simple you can use one-hot encoding (google it!), but be careful to eliminate one category from your one-hot encoding (why this?). What could go wrong if you encode categorical variables with more than two categories as numbers? \n",
    "\n",
    "- Train your model on the training data, using the features which you think are important. You can use penalty=None to train a simple unregularized model. Then make a prediction on the test data. You can submit your results to [Kaggle](https://www.kaggle.com/c/titanic) to get your accuracy score and see how good your model is. Using as many variables as possible can help you to get a good training accuracy, but this doesn't necessarily mean that your model will generalize well to the test-set! Finding a good model usually takes some insight into the data and problem, as well as machine learning skill. You can also use cross-validation with the training data to pick a good model before going to the test set. \n",
    "\n",
    "If you complete these tasks very quickly and would like to go further, you can try the following bonus tasks.\n",
    "\n",
    "- Bonus task 1: Try out some feature engineering. Make a new data column in the training data by using transformations of existing columns. Ratios of columns, log transforms, and power transforms (e.g. $x^2$) are all popular choices that you can play with. Can you improve your test-set classification accuracy by feature engineering?\n",
    "\n",
    "- Bonus task 2: Implement your own logistic regression model using numpy. You can use scipy.optimize to train your model using the method of maximum likelihood. To do this you will need to solve the following optimization problem\n",
    "\n",
    "$\\max_{\\beta} l(\\beta, Y, X)$\n",
    "\n",
    "where the log-likelihood function $l$ is given by\n",
    "\n",
    "$l(\\beta, Y, X) = \\sum_{i =1}^{N_{data}} y_i \\log(\\sigma(x_i\\cdotp \\beta)) + (1 -y_i) \\log(1 - \\sigma(x_i\\cdotp \\beta))$.\n",
    "\n",
    "You can derive this function by taking the log of the likelihood\n",
    "\n",
    "$L(\\beta, Y, X) = \\prod_{i =1}^{N_{data}} P(y_i = \\hat{y_i}| \\beta, X)$,\n",
    "\n",
    "where $\\hat{y_i}$ is the predicted outcome, and $y_i$ the true outcome.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>B42</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>C148</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1         0       3   \n",
       "1              2         1       1   \n",
       "2              3         1       3   \n",
       "3              4         1       1   \n",
       "4              5         0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887         0       2   \n",
       "887          888         1       1   \n",
       "888          889         0       3   \n",
       "889          890         1       1   \n",
       "890          891         0       3   \n",
       "\n",
       "                                                  Name     Sex   Age  SibSp  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                             Allen, Mr. William Henry    male  35.0      0   \n",
       "..                                                 ...     ...   ...    ...   \n",
       "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
       "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
       "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
       "\n",
       "     Parch            Ticket     Fare Cabin Embarked  \n",
       "0        0         A/5 21171   7.2500   NaN        S  \n",
       "1        0          PC 17599  71.2833   C85        C  \n",
       "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3        0            113803  53.1000  C123        S  \n",
       "4        0            373450   8.0500   NaN        S  \n",
       "..     ...               ...      ...   ...      ...  \n",
       "886      0            211536  13.0000   NaN        S  \n",
       "887      0            112053  30.0000   B42        S  \n",
       "888      2        W./C. 6607  23.4500   NaN        S  \n",
       "889      0            111369  30.0000  C148        C  \n",
       "890      0            370376   7.7500   NaN        Q  \n",
       "\n",
       "[891 rows x 12 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_train = pd.read_csv(\"data/titanic/train.csv\")\n",
    "titanic_test = pd.read_csv(\"data/titanic/test.csv\")\n",
    "titanic_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Parch</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Female</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>29.699118</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Age  Parch  SibSp  Female  Pclass_2  Pclass_3\n",
       "0    22.000000      0      1       0         0         1\n",
       "1    38.000000      0      1       1         0         0\n",
       "2    26.000000      0      0       1         0         1\n",
       "3    35.000000      0      1       1         0         0\n",
       "4    35.000000      0      0       0         0         1\n",
       "..         ...    ...    ...     ...       ...       ...\n",
       "886  27.000000      0      0       0         1         0\n",
       "887  19.000000      0      0       1         0         0\n",
       "888  29.699118      2      1       1         0         1\n",
       "889  26.000000      0      0       0         0         0\n",
       "890  32.000000      0      0       0         0         1\n",
       "\n",
       "[891 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#Here I have made some simple choices, but you can make this as complex as you want.\n",
    "#For some competitive solutions check out the \"code\" section of the Kaggle Titanic competition. \n",
    "feature_cols = [\"Sex\", \"Age\", \"Pclass\", \"Parch\", \"SibSp\"]\n",
    "\n",
    "def recode_data(df_raw):\n",
    "    \"\"\"Here I have done the encoding manually so that you can see it. \n",
    "       But for convenience when you build your own models \n",
    "       you can use the sklearn preprocessing tools. \"\"\"\n",
    "\n",
    "    df_encoded = df_raw.copy()\n",
    "    df_encoded[\"Female\"] = (df_encoded[\"Sex\"] == \"female\").astype(int)\n",
    "\n",
    "    df_encoded[\"Pclass_2\"] = (df_encoded[\"Pclass\"] == 2).astype(int)\n",
    "    df_encoded[\"Pclass_3\"] = (df_encoded[\"Pclass\"] == 3).astype(int)\n",
    "    #Pclass = 1 is not encoded as it would introduce a linear dependence. \n",
    "    #Pclass = 1 corresponds to Pclass_2 and Pclass_3 = 0\n",
    "\n",
    "    df_encoded = df_encoded.drop(columns = [\"Sex\", \"Pclass\"])\n",
    "    #Fill in missing ages with the mean age\n",
    "    df_encoded.loc[df_encoded.isnull().any(axis=1), \"Age\"] = df_encoded.loc[~df_encoded.isnull().any(axis=1), \"Age\"].mean()\n",
    "    return df_encoded\n",
    "\n",
    "\n",
    "X_encoded = recode_data(titanic_train[feature_cols])\n",
    "X_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy (10-folds) = 0.787\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "Nfolds = 10\n",
    "Y_train = titanic_train[\"Survived\"]\n",
    "\n",
    "clf = LogisticRegression(penalty = \"none\")\n",
    "cv_accuracy = cross_val_score(clf, X_encoded, Y_train, cv = Nfolds)\n",
    "print(\"Mean Accuracy ({}-folds) = {:.3f}\".format(Nfolds, cv_accuracy.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train on the entire training set and make a prediction\n",
    "clf.fit(X_encoded, Y_train)\n",
    "Y_hat = clf.predict(recode_data(titanic_test[feature_cols]))\n",
    "\n",
    "output = pd.DataFrame(titanic_test[\"PassengerId\"])\n",
    "output[\"Survived\"] = Y_hat\n",
    "output.to_csv(\"my_titanic_predictions.csv\",\n",
    "              index = False)\n",
    "\n",
    "#When I submit this result to Kaggle I get an accuracy of 0.75119."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do my predictions match those from SK_learn? True\n"
     ]
    }
   ],
   "source": [
    "#Custom Logistic Regression Model for bonus task #2\n",
    "from scipy.optimize import minimize\n",
    "from functools import partial\n",
    "\n",
    "class MyLogisticRegression(object):\n",
    "    def __init__(self):\n",
    "        self.beta = None\n",
    "        \n",
    "    def fit(self, Y, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy[\"intercept\"] = 1.0\n",
    "        X_copy = X_copy.astype(float)\n",
    "        beta0 = np.zeros(X_copy.shape[1])\n",
    "        \n",
    "        options ={\"iprint\": 2,\n",
    "                  \"gtol\": 1.0e-4,\n",
    "                  \"maxiter\": 100}\n",
    "        \n",
    "        llh_partial = partial(self.llh,\n",
    "                              Y=Y.values,\n",
    "                              X=X_copy)\n",
    "        \n",
    "        nllh_partial = lambda beta: -1*llh_partial(beta)\n",
    "        \n",
    "        sol = minimize(nllh_partial,\n",
    "                       beta0,\n",
    "                       method = \"L-BFGS-B\",\n",
    "                       jac = False,\n",
    "                       options=options)\n",
    "        self.beta = sol.x\n",
    "    \n",
    "    def llh(self, beta, Y, X):\n",
    "        p = self.predict_probability(beta, X)\n",
    "        \n",
    "        #To avoid problems with inf and nan restrict the log transformations\n",
    "        #to where the arguments will be away from 0.\n",
    "        return np.log(Y[Y ==1]*p[Y ==1]).sum() + np.log((1 - Y[Y==0])*(1 -p[Y==0])).sum()\n",
    "        \n",
    "    def predict_probability(self, beta, X):\n",
    "        mu = np.dot(X, beta)\n",
    "        return 1.0/(1 + np.exp(-mu))\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_copy = X.copy()\n",
    "        X_copy[\"intercept\"] = 1.0\n",
    "        p = self.predict_probability(self.beta, X_copy.values)\n",
    "        return (p > 0.5).astype(int)\n",
    "    \n",
    "mylogit = MyLogisticRegression()\n",
    "mylogit.fit(Y_train, X_encoded)\n",
    "my_Yhat = mylogit.predict(recode_data(titanic_test[feature_cols]))\n",
    "print(\"Do my predictions match those from SK_learn?\", (my_Yhat == Y_hat).all())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
